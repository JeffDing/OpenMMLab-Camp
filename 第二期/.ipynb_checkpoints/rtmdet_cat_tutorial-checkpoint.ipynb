{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wts2Z9JIrh-_"
   },
   "source": [
    "# 什么是目标检测\n",
    "\n",
    "\n",
    "目标检测任务即：给定图片，网络预测出所关注的所有物体边界框和类别。其是一个典型的多任务学习，既包括物体边界框学习还包括类别学习。\n",
    "\n",
    "# 什么是 MMDetection\n",
    "\n",
    "MMDetection 是被广泛使用的检测工具箱，包括了目标检测、实例分割、全景分割等多个通用检测方向，并支持了 75+ 个主流和前沿模型，为用户提供超过 440+ 个预训练模型，在学术研究和工业落地中拥有广泛应用。该框架的主要特点为：\n",
    "\n",
    "- **模块化设计**\n",
    "\n",
    "  MMDetection 将检测框架解耦成不同的模块组件，通过组合不同的模块组件，用户可以便捷地构建自定义的检测模型\n",
    "\n",
    "- **支持多种检测任务**\n",
    "\n",
    "  MMDetection 支持了各种不同的检测任务，包括**目标检测**，**实例分割**，**全景分割**，以及**半监督目标检测**。后续会重点支持多模态通用检测方向\n",
    "\n",
    "- **速度快**\n",
    "\n",
    "  基本的框和 mask 操作都实现了 GPU 版本，训练速度比其他代码库更快或者相当。\n",
    "\n",
    "- **性能高**\n",
    "\n",
    "  MMDetection 这个算法库源自于 COCO 2018 目标检测竞赛的冠军团队 *MMDet* 团队开发的代码，我们在之后持续进行了改进和提升。\n",
    "  新发布的 [RTMDet](configs/rtmdet) 还在实时实例分割和旋转目标检测任务中取得了最先进的成果，同时也在目标检测模型中取得了最佳的的参数量和精度平衡。\n",
    " \n",
    "**近期我们发布了 MMDetection 3.0 正式版本。在 MMDetection V2.0 基础上，通过更细粒度的模块解耦，我们进一步拆解出了数据、数据变换、模型、评测、可视化器等抽象，并将这些接口进行了统一设计，统一的数据流和细粒度的模块大幅提升了任务拓展性能。我们基于全新训练引擎 MMEngine 和计算机视觉的基础库 MMCV 进行了全面适配，经过对模型各个组件的重构和优化，全面提升了 MMDetection 的速度和精度，达到了现有检测框架中的最优水平。**\n",
    "\n",
    " \n",
    "MMDetection Repo：https://github.com/open-mmlab/mmdetection/\n",
    "\n",
    "MMDetection 官方文档链接：https://mmdetection.readthedocs.io/en/latest/\n",
    "\n",
    "\n",
    "下面我们将以 MMDetection 团队自研的 RTMDet 算法为例，结合一个简单的 cat 数据集来描述整个训练推理可视化过程。\n",
    "\n",
    "本教程一共包括如下流程：\n",
    "\n",
    "1. 数据集准备和可视化\n",
    "2. 自定义配置文件\n",
    "3. 训练前可视化验证\n",
    "4. 模型训练\n",
    "5. 模型测试和推理\n",
    "6. 可视化分析\n",
    "\n",
    "**注意事项：为了方便大家复现，本教程配套的 Notebook 将位于 https://github.com/open-mmlab/mmdetection/tree/tutorials 处。**\n",
    "\n",
    "在开始前，需要先检测环境并安装 MMDetection 及其依赖\n",
    "\n",
    "# 0 环境检测和安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gS4VUn0esAPF",
    "outputId": "9cfe63b4-ee0b-45df-9e1b-b83c87077356"
   },
   "outputs": [],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoqOolv5tH1p",
    "outputId": "2810fc52-81d3-40ca-890a-520e24b898c6"
   },
   "outputs": [],
   "source": [
    "# 安装 mmengine 和 mmcv 依赖\n",
    "# 为了防止后续版本变更导致的代码无法运行，我们暂时锁死版本\n",
    "!pwd\n",
    "%pip install -U \"openmim==0.3.7\"\n",
    "!mim install \"mmengine==0.7.1\"\n",
    "!mim install \"mmcv==2.0.0\"\n",
    "\n",
    "# Install mmdetection\n",
    "!rm -rf mmdetection\n",
    "# 为了防止后续更新导致的可能无法运行，特意新建了 tutorials 分支\n",
    "!git clone -b tutorials https://github.com/open-mmlab/mmdetection.git\n",
    "%cd mmdetection\n",
    "\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKMrVfgC1DAA",
    "outputId": "12657500-dc13-4e2b-f161-8bf71d48406a"
   },
   "outputs": [],
   "source": [
    "from mmengine.utils import get_git_hash\n",
    "from mmengine.utils.dl_utils import collect_env as collect_base_env\n",
    "\n",
    "import mmdet\n",
    "\n",
    "# 环境信息收集和打印\n",
    "def collect_env():\n",
    "    \"\"\"Collect the information of the running environments.\"\"\"\n",
    "    env_info = collect_base_env()\n",
    "    env_info['MMDetection'] = f'{mmdet.__version__}+{get_git_hash()[:7]}'\n",
    "    return env_info\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name, val in collect_env().items():\n",
    "        print(f'{name}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fxu3zQ0ptt_5"
   },
   "source": [
    "# 1 数据集准备和可视化\n",
    "\n",
    "我们提供了一个简单的 cat 猫数据集，该数据集来自社区用户，总共包括 144 张图片，并且已经提前划分为了训练集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLQXz6QWtvAI",
    "outputId": "22bc1b35-b719-45f3-8fb8-91de085f3a2e"
   },
   "outputs": [],
   "source": [
    "# 数据集下载\n",
    "!rm -rf cat_dataset*\n",
    "!wget https://download.openmmlab.com/mmyolo/data/cat_dataset.zip\n",
    "!unzip cat_dataset.zip -d cat_dataset && rm cat_dataset.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "YNYvdTTXvOVi",
    "outputId": "6247fd3c-9a0c-410a-8aff-53c4e6dd5228"
   },
   "outputs": [],
   "source": [
    "# 数据集可视化\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "image_paths= [filename for filename in os.listdir('cat_dataset/images')][:8]\n",
    "\n",
    "for i,filename in enumerate(image_paths):\n",
    "    name = os.path.splitext(filename)[0]\n",
    "\n",
    "    image = Image.open('cat_dataset/images/'+filename).convert(\"RGB\")\n",
    "  \n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{filename}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2IUzqNdvh9i"
   },
   "source": [
    "我们也提供了 coco json 标注，用户可以直接使用，而无需自己重新标注。COCO Json 可视化如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "LH0DPnR6viqB",
    "outputId": "dcec492d-0eaf-474f-fb35-39891213c83e"
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def apply_exif_orientation(image):\n",
    "    _EXIF_ORIENT = 274\n",
    "    if not hasattr(image, 'getexif'):\n",
    "        return image\n",
    "\n",
    "    try:\n",
    "        exif = image.getexif()\n",
    "    except Exception:\n",
    "        exif = None\n",
    "\n",
    "    if exif is None:\n",
    "        return image\n",
    "\n",
    "    orientation = exif.get(_EXIF_ORIENT)\n",
    "\n",
    "    method = {\n",
    "        2: Image.FLIP_LEFT_RIGHT,\n",
    "        3: Image.ROTATE_180,\n",
    "        4: Image.FLIP_TOP_BOTTOM,\n",
    "        5: Image.TRANSPOSE,\n",
    "        6: Image.ROTATE_270,\n",
    "        7: Image.TRANSVERSE,\n",
    "        8: Image.ROTATE_90,\n",
    "    }.get(orientation)\n",
    "    if method is not None:\n",
    "        return image.transpose(method)\n",
    "    return image\n",
    "\n",
    "\n",
    "def show_bbox_only(coco, anns, show_label_bbox=True, is_filling=True):\n",
    "    \"\"\"Show bounding box of annotations Only.\"\"\"\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    image2color = dict()\n",
    "    for cat in coco.getCatIds():\n",
    "        image2color[cat] = (np.random.random((1, 3)) * 0.7 + 0.3).tolist()[0]\n",
    "\n",
    "    polygons = []\n",
    "    colors = []\n",
    "\n",
    "    for ann in anns:\n",
    "        color = image2color[ann['category_id']]\n",
    "        bbox_x, bbox_y, bbox_w, bbox_h = ann['bbox']\n",
    "        poly = [[bbox_x, bbox_y], [bbox_x, bbox_y + bbox_h],\n",
    "                [bbox_x + bbox_w, bbox_y + bbox_h], [bbox_x + bbox_w, bbox_y]]\n",
    "        polygons.append(Polygon(np.array(poly).reshape((4, 2))))\n",
    "        colors.append(color)\n",
    "\n",
    "        if show_label_bbox:\n",
    "            label_bbox = dict(facecolor=color)\n",
    "        else:\n",
    "            label_bbox = None\n",
    "\n",
    "        ax.text(\n",
    "            bbox_x,\n",
    "            bbox_y,\n",
    "            '%s' % (coco.loadCats(ann['category_id'])[0]['name']),\n",
    "            color='white',\n",
    "            bbox=label_bbox)\n",
    "\n",
    "    if is_filling:\n",
    "        p = PatchCollection(\n",
    "            polygons, facecolor=colors, linewidths=0, alpha=0.4)\n",
    "        ax.add_collection(p)\n",
    "    p = PatchCollection(\n",
    "        polygons, facecolor='none', edgecolors=colors, linewidths=2)\n",
    "    ax.add_collection(p)\n",
    "\n",
    "    \n",
    "coco = COCO('cat_dataset/annotations/test.json')\n",
    "image_ids = coco.getImgIds()\n",
    "np.random.shuffle(image_ids)\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# 只可视化 8 张图片\n",
    "for i in range(8):\n",
    "    image_data = coco.loadImgs(image_ids[i])[0]\n",
    "    image_path = osp.join('cat_dataset/images/',image_data['file_name'])\n",
    "    annotation_ids = coco.getAnnIds(\n",
    "            imgIds=image_data['id'], catIds=[], iscrowd=0)\n",
    "    annotations = coco.loadAnns(annotation_ids)\n",
    "    \n",
    "    ax = plt.subplot(2, 4, i+1)\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # 这行代码很关键，否则可能图片和标签对不上\n",
    "    image=apply_exif_orientation(image)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    show_bbox_only(coco, annotations)\n",
    "    \n",
    "    plt.title(f\"{filename}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "        \n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I_Gq5c5v-s7"
   },
   "source": [
    "# 2 自定义配置文件\n",
    "\n",
    "本教程采用 RTMDet 进行演示，在开始自定义配置文件前，先来了解下 RTMDet 算法。\n",
    "\n",
    "![RTMDet](https://user-images.githubusercontent.com/27466624/225922103-404064c1-3cb0-4ab5-9388-79f9517dcdb0.jpg)\n",
    "\n",
    "其模型架构图如上所示。RTMDet 是一个高性能低延时的检测算法，目前已经实现了目标检测、实例分割和旋转框检测任务。其简要描述为：**为了获得更高效的模型架构，MMDetection 探索了一种具有骨干和 Neck 兼容容量的架构，由一个基本的构建块构成，其中包含大核深度卷积。MMDetection 进一步在动态标签分配中计算匹配成本时引入软标签，以提高准确性。结合更好的训练技巧，得到的目标检测器名为 RTMDet，在 NVIDIA 3090 GPU 上以超过 300 FPS 的速度实现了 52.8% 的 COCO AP，优于当前主流的工业检测器。RTMDet 在小/中/大/特大型模型尺寸中实现了最佳的参数-准确度权衡，适用于各种应用场景，并在实时实例分割和旋转对象检测方面取得了新的最先进性能。**\n",
    "\n",
    "|                            | mAP             | Params         | Flops        | Inference speed |\n",
    "| -------------------------- | --------------- | -------------- | ------------ | --------------- |\n",
    "| Baseline(YOLOX)            | 40.2            | 9M             | 13.4G        | 1.2ms           |\n",
    "| + AdamW + Flat Cosine      | 40.6 (+0.4)     | 9M             | 13.4G        | 1.2ms           |\n",
    "| + CSPNeXt backbone & PAFPN | 41.8 (+1.2)     | 10.07M (+1.07) | 14.8G (+1.4) | 1.22ms (+0.02)  |\n",
    "| + SepBNHead                | 41.8 (+0)       | 8.89M (-1.18)  | 14.8G        | 1.22ms          |\n",
    "| + Label Assign & Loss      | 42.9 (+1.1)     | 8.89M          | 14.8G        | 1.22ms          |\n",
    "| + Cached Mosaic & MixUp    | 44.2 (+1.3)     | 8.89M          | 14.8G        | 1.22ms          |\n",
    "| + RSB-pretrained backbone  | **44.5 (+0.3)** | 8.89M          | 14.8G        | 1.22ms          |\n",
    "\n",
    "详情见 [论文](https://arxiv.org/abs/2212.07784)。\n",
    "\n",
    "cat 是一个单类的数据集，而 MMDetection 中提供的是 COCO 80 类配置，因此我们需要对一些重要参数通过配置来修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXyL9UOqwGuC"
   },
   "outputs": [],
   "source": [
    "# 当前路径位于 mmdetection/tutorials, 配置将写到 mmdetection/tutorials 路径下\n",
    "\n",
    "config_cat = \"\"\"\n",
    "_base_ = 'configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py'\n",
    "\n",
    "data_root = 'cat_dataset/'\n",
    "\n",
    "# 非常重要\n",
    "metainfo = {\n",
    "    # 类别名，注意 classes 需要是一个 tuple，因此即使是单类，\n",
    "    # 你应该写成 `cat,` 很多初学者经常会在这犯错\n",
    "    'classes': ('cat',),\n",
    "    'palette': [\n",
    "        (220, 20, 60),\n",
    "    ]\n",
    "}\n",
    "num_classes = 1\n",
    "\n",
    "# 训练 40 epoch\n",
    "max_epochs = 40\n",
    "# 训练单卡 bs= 12\n",
    "train_batch_size_per_gpu = 12\n",
    "# 可以根据自己的电脑修改\n",
    "train_num_workers = 4\n",
    "\n",
    "# 验证集 batch size 为 1\n",
    "val_batch_size_per_gpu = 1\n",
    "val_num_workers = 2\n",
    "\n",
    "# RTMDet 训练过程分成 2 个 stage，第二个 stage 会切换数据增强 pipeline\n",
    "num_epochs_stage2 = 5\n",
    "\n",
    "# batch 改变了，学习率也要跟着改变， 0.004 是 8卡x32 的学习率\n",
    "base_lr = 12 * 0.004 / (32*8)\n",
    "\n",
    "# 采用 COCO 预训练权重\n",
    "load_from = 'https://download.openmmlab.com/mmdetection/v3.0/rtmdet/rtmdet_tiny_8xb32-300e_coco/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'  # noqa\n",
    "\n",
    "model = dict(\n",
    "    # 考虑到数据集太小，且训练时间很短，我们把 backbone 完全固定\n",
    "    # 用户自己的数据集可能需要解冻 backbone\n",
    "    backbone=dict(frozen_stages=4),\n",
    "    # 不要忘记修改 num_classes\n",
    "    bbox_head=dict(dict(num_classes=num_classes)))\n",
    "\n",
    "# 数据集不同，dataset 输入参数也不一样\n",
    "train_dataloader = dict(\n",
    "    batch_size=train_batch_size_per_gpu,\n",
    "    num_workers=train_num_workers,\n",
    "    pin_memory=False,\n",
    "    dataset=dict(\n",
    "        data_root=data_root,\n",
    "        metainfo=metainfo,\n",
    "        ann_file='annotations/trainval.json',\n",
    "        data_prefix=dict(img='images/')))\n",
    "\n",
    "val_dataloader = dict(\n",
    "    batch_size=val_batch_size_per_gpu,\n",
    "    num_workers=val_num_workers,\n",
    "    dataset=dict(\n",
    "        metainfo=metainfo,\n",
    "        data_root=data_root,\n",
    "        ann_file='annotations/test.json',\n",
    "        data_prefix=dict(img='images/')))\n",
    "\n",
    "test_dataloader = val_dataloader\n",
    "\n",
    "# 默认的学习率调度器是 warmup 1000，但是 cat 数据集太小了，需要修改 为 30 iter\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='LinearLR',\n",
    "        start_factor=1.0e-5,\n",
    "        by_epoch=False,\n",
    "        begin=0,\n",
    "        end=30),\n",
    "    dict(\n",
    "        type='CosineAnnealingLR',\n",
    "        eta_min=base_lr * 0.05,\n",
    "        begin=max_epochs // 2,  # max_epoch 也改变了\n",
    "        end=max_epochs,\n",
    "        T_max=max_epochs // 2,\n",
    "        by_epoch=True,\n",
    "        convert_to_iter_based=True),\n",
    "]\n",
    "optim_wrapper = dict(optimizer=dict(lr=base_lr))\n",
    "\n",
    "# 第二 stage 切换 pipeline 的 epoch 时刻也改变了\n",
    "_base_.custom_hooks[1].switch_epoch = max_epochs - num_epochs_stage2\n",
    "\n",
    "val_evaluator = dict(ann_file=data_root + 'annotations/test.json')\n",
    "test_evaluator = val_evaluator\n",
    "\n",
    "# 一些打印设置修改\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(interval=10, max_keep_ckpts=2, save_best='auto'),  # 同时保存最好性能权重\n",
    "    logger=dict(type='LoggerHook', interval=5))\n",
    "train_cfg = dict(max_epochs=max_epochs, val_interval=10)\n",
    "\"\"\"\n",
    "\n",
    "with open('rtmdet_tiny_1xb12-40e_cat.py', 'w') as f:\n",
    "    f.write(config_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x42D5iYZ0bLq"
   },
   "source": [
    "需要注意几个问题：\n",
    "\n",
    "1. 自定义数据集中最重要的是 metainfo 字段，用户在配置完成后要记得将其传给 dataset，否则不生效(有些用户在自定义数据集时候喜欢去 直接修改 coco.py 源码，这个是强烈不推荐的做法，正确做法是配置 metainfo 并传给 dataset)\n",
    "\n",
    "2. 如果用户 metainfo 配置不正确，通常会出现几种情况：(1) 出现 num_classes 不匹配错误 (2) loss_bbox 始终为 0 (3) 出现训练后评估结果为空等典型情况\n",
    "\n",
    "3. MMDetection 提供的学习率大部分都是基于 8 卡，如果你的总 bs 不同，一定要记得缩放学习率，否则有些算法很容易出现 NAN，具体参考 https://mmdetection.readthedocs.io/zh_CN/latest/user_guides/train.html#id3\n",
    "\n",
    "# 3 训练前可视化验证\n",
    "\n",
    "我们可以采用 mmdet 提供的 `tools/analysis_tools/browse_dataset.py` 脚本来对训练前的 dataloader 输出进行可视化，确保数据部分没有问题。\n",
    "考虑到我们仅仅想可视化前几张图片，因此下面基于 browse_dataset.py 实现一个简单版本即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "9HD30AlN0cRS",
    "outputId": "336d932f-2a9b-4a24-e985-92083b2c904b"
   },
   "outputs": [],
   "source": [
    "from mmdet.registry import DATASETS, VISUALIZERS\n",
    "from mmengine.config import Config\n",
    "from mmengine.registry import init_default_scope\n",
    "\n",
    "cfg = Config.fromfile('rtmdet_tiny_1xb12-40e_cat.py')\n",
    "\n",
    "init_default_scope(cfg.get('default_scope', 'mmdet'))\n",
    "\n",
    "dataset = DATASETS.build(cfg.train_dataloader.dataset)\n",
    "visualizer = VISUALIZERS.build(cfg.visualizer)\n",
    "visualizer.dataset_meta = dataset.metainfo\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# 只可视化前 8 张图片\n",
    "for i in range(8):\n",
    "   item=dataset[i]\n",
    "\n",
    "   img = item['inputs'].permute(1, 2, 0).numpy()\n",
    "   data_sample = item['data_samples'].numpy()\n",
    "   gt_instances = data_sample.gt_instances\n",
    "   img_path = osp.basename(item['data_samples'].img_path)\n",
    "\n",
    "   gt_bboxes = gt_instances.get('bboxes', None)\n",
    "   gt_instances.bboxes = gt_bboxes.tensor\n",
    "   data_sample.gt_instances = gt_instances\n",
    "\n",
    "   visualizer.add_datasample(\n",
    "            osp.basename(img_path),\n",
    "            img,\n",
    "            data_sample,\n",
    "            draw_pred=False,\n",
    "            show=False)\n",
    "   drawed_image=visualizer.get_image()\n",
    "\n",
    "   plt.subplot(2, 4, i+1)\n",
    "   plt.imshow(drawed_image[..., [2, 1, 0]])\n",
    "   plt.title(f\"{osp.basename(img_path)}\")\n",
    "   plt.xticks([])\n",
    "   plt.yticks([])\n",
    "\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD59jaYY3veC"
   },
   "source": [
    "# 4 模型训练\n",
    "\n",
    "在验证数据流本身没有问题后，就可以开始进行训练了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiSxY1U831Ov",
    "outputId": "01a636bc-0fcb-406d-aaef-d6d836b80dbc"
   },
   "outputs": [],
   "source": [
    "!python tools/train.py rtmdet_tiny_1xb12-40e_cat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz711UQt4IRN"
   },
   "source": [
    "由于 google colab 的 IO 读取 data_time 时间很长，训练大概需要 15 分钟。实际上在低端 1660 显卡的本地环境上运行大概只需要 5 分钟。\n",
    "\n",
    "运行完成后，会在当前路径下生成 work_dirs/rtmdet_tiny_1xb12-40e_cat 路径，内部存放了 log 和权重文件。\n",
    "\n",
    "可以发现最佳性能为第 30 epoch 时候，性能为 85.2 mAP (注： 由于训练 epoch 很短，因此多次运行可能 mAP 会有 3~4 个点的差异)。\n",
    "\n",
    "```text\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.852\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.986\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.922\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.852\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.850\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.920\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.920\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.920\n",
    "```\n",
    "\n",
    "# 5 模型测试和推理\n",
    "如果你想离线测试，则可以使用我们提供的 test.py 脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nN840uwB88IM",
    "outputId": "53e00cfe-0fa1-4ecc-e554-e44a8848717a"
   },
   "outputs": [],
   "source": [
    "!python tools/test.py rtmdet_tiny_1xb12-40e_cat.py work_dirs/rtmdet_tiny_1xb12-40e_cat/best_coco/bbox_mAP_epoch_30.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbxgThf-9YdE"
   },
   "source": [
    "在测试阶段，你可以设置 --show-dir 将测试图片的真实值和预测值保存下来，然后进行深入分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZ5umkLA9wZT",
    "outputId": "ed439bc0-a4b2-4176-c972-d0d78736cb3b"
   },
   "outputs": [],
   "source": [
    "!python tools/test.py rtmdet_tiny_1xb12-40e_cat.py work_dirs/rtmdet_tiny_1xb12-40e_cat/best_coco/bbox_mAP_epoch_30.pth --show-dir results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfI3XvIm-JJn"
   },
   "source": [
    "会在 `work_dir/rtmdet_tiny_1xb12-40e_cat/当前时间戳/results/` 下生成测试图片，下面对前 8 张图片进行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hwworoFc-W-Y",
    "outputId": "f752d085-ccee-40bf-c5f9-2208c5819599"
   },
   "outputs": [],
   "source": [
    "# 数据集可视化\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# 你如果重新跑，这个时间戳是不一样的，需要自己修改\n",
    "root_path='work_dirs/rtmdet_tiny_1xb12-40e_cat/20230517_120933/results/'\n",
    "image_paths= [filename for filename in os.listdir(root_path)][:4]\n",
    "\n",
    "for i,filename in enumerate(image_paths):\n",
    "    name = os.path.splitext(filename)[0]\n",
    "\n",
    "    image = Image.open(root_path+filename).convert(\"RGB\")\n",
    "  \n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{filename}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoaJHJkzFVfL"
   },
   "source": [
    "左边显示的是标注框，右边显示的是预测框。\n",
    "\n",
    "实际上 MMDetection 支持多种可视化后端，例如 TensorBoard 和 WandB，默认情况下是将图片保存到本地，用户只需要修改可视化部分配置即可轻松切换。如下所示\n",
    "\n",
    "以下配置只需要加到 `rtmdet_tiny_1xb12-40e_cat.py` 配置最后即可\n",
    "\n",
    "**(1) 同时使用本地和 WandB 后端**\n",
    "\n",
    "```python\n",
    "visualizer = dict(vis_backends = [dict(type='LocalVisBackend'), dict(type='WandbVisBackend')])\n",
    "```\n",
    "\n",
    "**(2) 仅仅使用 TensorBoard 后端**\n",
    "\n",
    "```python\n",
    "visualizer = dict(vis_backends = [dict(type='TensorboardVisBackend')])\n",
    "```\n",
    "修改配置后重新运行 test.py 即可在 WandB 和 TensorBoard 前端界面查看 图片和 log 等。\n",
    "\n",
    "如果想对单张图片进行推理，你可以直接使用 `mmdetection/demo/image_demo.py` 脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPU1GNxoICgf",
    "outputId": "77424683-3837-4f0b-f19e-3e971628f59a"
   },
   "outputs": [],
   "source": [
    "!python demo/image_demo.py cat_dataset/images/IMG_20211102_003429.jpg rtmdet_tiny_1xb12-40e_cat.py --weights work_dirs/rtmdet_tiny_1xb12-40e_cat/best_coco/bbox_mAP_epoch_30.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KA_BPPAfIqQ3",
    "outputId": "31c4f8d8-70b6-41c4-af59-e9a7d12f1e53"
   },
   "outputs": [],
   "source": [
    "Image.open('outputs/vis/IMG_20211102_003429.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLuD4nANJEdP"
   },
   "source": [
    "# 6 可视化分析\n",
    "\n",
    "可视化分析包括特征图可视化以及类似 Grad CAM 等可视化分析手段。不过由于 MMDetection 中还没有实现，我们可以直接采用 MMYOLO 中提供的功能和脚本。MMYOLO 是基于 MMDetection 开发，并且此案有了统一的代码组织形式，因此 MMDetection 配置可以直接在 MMYOLO 中使用，无需更改配置。\n",
    "\n",
    "## MMYOLO 环境和依赖安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZMy9CkqKdhy",
    "outputId": "09d4aee0-a441-4cfc-f95c-289ea567238f"
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "%cd ../\n",
    "!rm -rf mmyolo\n",
    "# 为了防止后续更新导致的可能无法运行，特意新建了 tutorials 分支\n",
    "!git clone -b tutorials https://github.com/open-mmlab/mmyolo.git \n",
    "%cd mmyolo\n",
    "\n",
    "%pip install -e .\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lT8ee-LwLY3j"
   },
   "source": [
    "## 特征图可视化\n",
    "\n",
    "MMYOLO 中，将使用 MMEngine 提供的 `Visualizer` 可视化器进行特征图可视化，其具备如下功能：\n",
    "\n",
    "- 支持基础绘图接口以及特征图可视化。\n",
    "- 支持选择模型中的不同层来得到特征图，包含 `squeeze_mean` ， `select_max` ， `topk` 三种显示方式，用户还可以使用 `arrangement` 自定义特征图显示的布局方式。\n",
    "\n",
    "你可以调用 `demo/featmap_vis_demo.py` 来简单快捷地得到可视化结果，为了方便理解，将其主要参数的功能梳理如下：\n",
    "\n",
    "- `img`：选择要用于特征图可视化的图片，支持单张图片或者图片路径列表。\n",
    "\n",
    "- `config`：选择算法的配置文件。\n",
    "\n",
    "- `checkpoint`：选择对应算法的权重文件。\n",
    "\n",
    "- `--out-file`：将得到的特征图保存到本地，并指定路径和文件名。\n",
    "\n",
    "- `--device`：指定用于推理图片的硬件，`--device cuda：0`  表示使用第 1 张 GPU 推理，`--device cpu` 表示用 CPU 推理。\n",
    "\n",
    "- `--score-thr`：设置检测框的置信度阈值，只有置信度高于这个值的框才会显示。\n",
    "\n",
    "- `--preview-model`：可以预览模型，方便用户理解模型的特征层结构。\n",
    "\n",
    "- `--target-layers`：对指定层获取可视化的特征图。\n",
    "\n",
    "  - 可以单独输出某个层的特征图，例如： `--target-layers backbone` ,  `--target-layers neck` ,  `--target-layers backbone.stage4` 等。\n",
    "  - 参数为列表时，也可以同时输出多个层的特征图，例如： `--target-layers backbone.stage4 neck` 表示同时输出 backbone 的 stage4 层和 neck 的三层一共四层特征图。\n",
    "\n",
    "- `--channel-reduction`：输入的 Tensor 一般是包括多个通道的，`channel_reduction` 参数可以将多个通道压缩为单通道，然后和图片进行叠加显示，有以下三个参数可以设置：\n",
    "\n",
    "  - `squeeze_mean`：将输入的 C 维度采用 mean 函数压缩为一个通道，输出维度变成 (1, H, W)。\n",
    "  - `select_max`：将输入先在空间维度 sum，维度变成 (C, )，然后选择值最大的通道。\n",
    "  - `None`：表示不需要压缩，此时可以通过 `topk` 参数可选择激活度最高的 `topk` 个特征图显示。\n",
    "\n",
    "- `--topk`：只有在 `channel_reduction` 参数为 `None` 的情况下， `topk` 参数才会生效，其会按照激活度排序选择 `topk` 个通道，然后和图片进行叠加显示，并且此时会通过 `--arrangement` 参数指定显示的布局，该参数表示为一个数组，两个数字需要以空格分开，例如： `--topk 5 --arrangement 2 3` 表示以 `2行 3列` 显示激活度排序最高的 5 张特征图， `--topk 7 --arrangement 3 3` 表示以 `3行 3列` 显示激活度排序最高的 7 张特征图。\n",
    "\n",
    "  - 如果 topk 不是 -1，则会按照激活度排序选择 topk 个通道显示。\n",
    "  - 如果 topk = -1，此时通道 C 必须是 1 或者 3 表示输入数据是图片，否则报错提示用户应该设置 `channel_reduction` 来压缩通道。\n",
    "\n",
    "- 考虑到输入的特征图通常非常小，函数默认将特征图进行上采样后方便进行可视化。\n",
    "\n",
    "**注意：当图片和特征图尺度不一样时候，`draw_featmap` 函数会自动进行上采样对齐。如果你的图片在推理过程中前处理存在类似 Pad 的操作此时得到的特征图也是 Pad 过的，那么直接上采样就可能会出现不对齐问题。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbRNoOtkXx4N"
   },
   "source": [
    "我们计划采用 cat_dataset/images/IMG_20211024_223313.jpg 图片进行后续可视化分析，不过由于这张图片分辨率过大，会导致程序奔溃，因此我们先将其缩放处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TK7q2BZZXbxy",
    "outputId": "2b8f87a8-296c-4bf9-835b-0364de003c21"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('../mmdetection/cat_dataset/images/IMG_20211024_223313.jpg')\n",
    "h,w=img.shape[:2]\n",
    "resized_img = cv2.resize(img, (640, 640))\n",
    "cv2.imwrite('resized_image.jpg', resized_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4LTwQO0ZDvu"
   },
   "source": [
    "**1. 可视化 backbone 输出的 3 个通道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ppe6g2MqMo9Y",
    "outputId": "2784e63b-bdb2-4c5d-9687-ab5ccfa85b09"
   },
   "outputs": [],
   "source": [
    "!python demo/featmap_vis_demo.py \\\n",
    "      resized_image.jpg \\\n",
    "      ../mmdetection/rtmdet_tiny_1xb12-40e_cat.py \\\n",
    "      ../mmdetection/work_dirs/rtmdet_tiny_1xb12-40e_cat/best_coco/bbox_mAP_epoch_30.pth  \\\n",
    "      --target-layers backbone  \\\n",
    "      --channel-reduction squeeze_mean\n",
    "Image.open('output/resized_image.jpg')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy2Pw_ldao-M"
   },
   "source": [
    "上图中绘制的 3 个输出特征图对应大中小输出特征图。由于本次训练的 backbone 实际上没有参与训练，从上图可以看到，大物体 cat 是在小特征图进行预测，这符合目标检测分层检测思想。\n",
    "\n",
    "**2. 可视化 neck 输出的 3 个通道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u0i8ScCIa0RZ",
    "outputId": "4a320bf3-c9fa-421c-9bb5-7542fe60e3f1"
   },
   "outputs": [],
   "source": [
    "!python demo/featmap_vis_demo.py \\\n",
    "      resized_image.jpg \\\n",
    "      ../mmdetection/rtmdet_tiny_1xb12-40e_cat.py \\\n",
    "      ../mmdetection/work_dirs/rtmdet_tiny_1xb12-40e_cat/best_coco/bbox_mAP_epoch_30.pth  \\\n",
    "      --target-layers neck  \\\n",
    "      --channel-reduction squeeze_mean\n",
    "Image.open('output/resized_image.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLi1_1V_a993"
   },
   "source": [
    "从上图可以看出，由于 neck 是参与训练的，并且 FPN 间的信息融合导致输出特征图更加聚集\n",
    "\n",
    "## Grad-Based CAM 可视化\n",
    "\n",
    "由于目标检测的特殊性，这里实际上可视化的并不是 CAM 而是 Grad Box AM。使用前需要先安装 grad-cam 库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2NsAzjHbffv",
    "outputId": "26410f28-bbb8-473c-ddc8-013834916c06"
   },
   "outputs": [],
   "source": [
    "!pip install \"grad-cam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsyeTLT7bpUc"
   },
   "source": [
    "**(a) 查看 neck 输出的最小输出特征图的 Grad CAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "id": "21N4PHN-brUM",
    "outputId": "993141d7-6524-4415-b058-80cd0850e0d7"
   },
   "outputs": [],
   "source": [
    "!python demo/boxam_vis_demo.py \\\n",
    "      resized_image.jpg \\\n",
    "      ../mmdetection/rtmdet_tiny_1xb12-40e_cat.py \\\n",
    "      ../mmdetection/work_dirs/rtmdet_tiny_1xb12-40e_cat/best_coco/bbox_mAP_epoch_30.pth  \\\n",
    "      --target-layer neck.out_convs[2]\n",
    "Image.open('output/resized_image.jpg')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EX77NdndEWd"
   },
   "source": [
    "可以看出效果较好\n",
    "\n",
    "**(b) 查看 neck 输出的最大输出特征图的 Grad CAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "id": "GMvGNEVSdDYX",
    "outputId": "9a8c7606-9242-442a-a8c3-9b90fc61fece"
   },
   "outputs": [],
   "source": [
    "!python demo/boxam_vis_demo.py \\\n",
    "      resized_image.jpg \\\n",
    "      ../mmdetection/rtmdet_tiny_1xb12-40e_cat.py \\\n",
    "      ../mmdetection/work_dirs/rtmdet_tiny_1xb12-40e_cat/best_coco/bbox_mAP_epoch_30.pth  \\\n",
    "      --target-layer neck.out_convs[0]\n",
    "Image.open('output/resized_image.jpg')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri7xKJF7dsbl"
   },
   "source": [
    "可以发现由于大物体不会在该层预测，因此梯度可视化是 0，符合预期。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlaggLA_fRqN"
   },
   "source": [
    "# 检测新趋势\n",
    "随着 ChatGPT 等的 LLM 飞速发展，传统的目标检测也逐渐发展为传统的封闭类别集合检测和自然语言相结合的开放类别检测(当然还有其他非常多结合检测的新方向)。典型的方向如：\n",
    "\n",
    "1. **Open-Vocabulary Object Detection**，即开放词汇目标检测，给定图片和类别词汇表，检测所有物体\n",
    "2. **Grounding Object Detection**，即给定图片和文本描述，预测文本中所提到的在图片中的物体位置\n",
    "\n",
    "大家可以多关注这个新方向和新趋势。该方向的典型算法为 GLIP, 目前(2023.6)在 MMDetection 的 dev-3.x 中已经支持，其检测效果演示如下(如果你感兴趣，可以自己跑一下)：\n",
    "\n",
    "**(1) 图片+ 语言描述：bench.car**\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/17425982/234548156-ef9bbc2e-7605-4867-abe6-048b8578893d.png)\n",
    "\n",
    "上述输入的是固定类别名，因此等价于 Open-Vocabulary Object Detection。如果将类别名设置为 COCO 全部类别，那么其实就可以转变了常规的目标检测算法。\n",
    "\n",
    "**(2) 图片+ 语言描述：There are a lot of cars here.**\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/17425982/234548490-d2e0a16d-1aad-4708-aea0-c829634fabbd.png)\n",
    "\n",
    "上述输入是自然语言描述，因此等价于 Grounding Object Detection\n",
    "\n",
    "当然随着 ChatGPT 的强大功能，一个模型可以完成非常多不可思议的事情，在视觉领域大家也开始倾向于研究大一统模型，例如通用图像分割模型，一个模型可以实现封闭集和开放集语义分割、实例分割、全景分割、图像描述等等任务，典型的如 X-Decoder\n",
    "\n",
    "![image](https://github.com/open-mmlab/mmdetection/assets/17425982/250772dd-d58c-4f04-8c2a-a2dd4eb88d11)\n",
    "\n",
    "目前 MMDetection 也在支持该算法。后续 MMDetection 会重点支持多模态算法，欢迎有志之士来共同参与！\n",
    "\n",
    "# 总结\n",
    "\n",
    "本教程提供了从数据到推理的全流程，并进行了详细的可视化分析，最后简单分析和探讨了下目标检测新的发展趋势。希望本文对你有帮助！！！"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
